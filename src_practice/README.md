# SGEMM CUDA Practice

This directory contains my practice implementations of SGEMM kernels.

## Directory Structure

```
src_practice/
├── kernels/           # Your kernel implementations
│   ├── 1_naive.cuh
│   └── ...
├── kernels.cuh        # Include all kernels (enable one by one)
├── runner.cu          # Copied from src/ (same as original)
└── runner.cuh
```

## How to Use

### 1. Build

```bash
# In project root directory
make                    # Build both sgemm and sgemm_practice
```

### 2. Run

```bash
# Quick test (build + run)
make practice KERNEL=1     # Test your practice version
make test KERNEL=1         # Test original (for comparison)
```

### 3. Implement a New Kernel

1. Create or edit `.cuh` file in `kernels/` directory
2. Uncomment corresponding `#include` in `kernels.cuh`
3. Reference original implementation in `../src/kernels/`
4. Build: `make`
5. Test: `./build/sgemm_practice <kernel_num>`

### 4. Compare with Original

```bash
# Performance comparison (quick)
make compare KERNEL=1              # Compare both versions
make compare KERNEL="1 2 3"        # Compare multiple kernels
make compare KERNEL="1 2" PRACTICE_ONLY=1  # Only run practice version
```

### 5. Full benchmark
```bash
make bench
```

## Kernel Progress

- [ ] Kernel 1: Naive
- [ ] Kernel 2: Global Memory Coalescing
- [ ] Kernel 3: Shared Memory Blocking
- [ ] Kernel 4: 1D Blocktiling
- [ ] Kernel 5: 2D Blocktiling
- [ ] Kernel 6: Vectorized Memory Access
- [ ] Kernel 7: Resolve Bank Conflicts (Linearize)
- [ ] Kernel 8: Resolve Bank Conflicts (Offset)
- [ ] Kernel 9: Autotuning
- [ ] Kernel 10: Warptiling
- [ ] Kernel 11: Double Buffering

## Kernels

### Kernel 1: Naive Implementation

1. Just translate the mathematics.
2. Each thread computes one element of C.

#### Hints

1. Understand `blockIdx`, `blockDim`, and `threadIdx` in CUDA programs.
2. Matrix dimension transformation: MxK * KxN = MxN
3. Global memory access: 2MNK

### Kernel 2: Global Memory Coalescing

1. Apply the idea of “warp coalesced memory access”.
2. Each thread still computes one element of C.

#### Hints

1. Understand warp coalesced memory access.
2. Ensure threads within a warp access consecutive addresses in B and C.
3. Global memory access: O(2MNK); fetches data faster than Kernel 1 due to coalesced memory access.

#### Notes

Why can “coalesced memory access” accelerate the execution？

Coalesced memory access accelerates execution because it reduces the number of memory transactions and thus overall memory traffic.

In a naive access pattern, the memory accesses generated by a warp are translated into many memory transactions (up to 32). Since each transaction typically transfers a full cache line (e.g., 128B), this results in fetching 128B × 32 bytes while only 4B × 32 bytes are useful.

With coalesced access, the same warp generates a single memory transaction, fetching 128B with nearly all bytes being useful, which significantly improves effective bandwidth utilization.

You may also ask: aren’t threads in a single warp accessing memory in parallel? Why is it still slow?

Although threads in a warp issue memory accesses in parallel, the memory system itself has limited service parallelism, constrained by channels, banks, and transaction pipelines.

When a warp generates many memory transactions, these transactions contend for the same memory resources and are partially serialized. Since a warp cannot proceed until all its memory transactions complete, this contention causes the entire warp to stall.

### Kernel 3: Shared Memory Blocking

1. Use shared memory to reuse global memory data and reduce global memory traffic.
2. Each thread still computes one element of C.

#### Hints

1. Based on Kernel 2.
2. For each thread block, iteratively load blocks (aka tiles) from A and B along the K dimension into shared memory.
3. Use `__syncthreads()` to ensure all threads in the block finish loading before computation. Similarly, use `__syncthreads()` after computation to ensure all threads finish using shared memory before loading new data.
4. With K-dimension tiling of size T (e.g., 32):
   - Global memory loads are reduced from O(2MNK) to O(2MNK / T).
   - Each element loaded from global memory is reused.
   - Shared memory accesses remain O(2MNK), but are much cheaper than global memory accesses.